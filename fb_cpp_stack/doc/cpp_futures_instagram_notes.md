# C++ Futures at Instagram Notes


This page records the summary and expansion of Instagram's tech blog: [C++ Futures at Instagram](https://instagram-engineering.com/c-futures-at-instagram-9628ff634f49).  

The following part of this article will first describe the evolution path of Instagram from simple synchronous I/O to none-blocking I/O to future version.  Then describe the common design pattern for supporting this and then lower level operating system support for I/O multiplexing.

## Instagram's  story

<img src="../resource/instagram_async_io.png" alt="instagram_async_io.png" width="400"/>


Django process model is a synchronous processing model with long latency, each process can only serve one request at a time.  To scale up network latency, Instagram use async IO for independent services and leverage the power of future->then to organize their code structure.  The picture below shows Instagram's infrastructure to support the two high-performing recommendation services: suggested users and chaining


<img src="../resource/instagram_service_infra.png" alt="instagram_service_infra.png" width="400"/>

Frontend(Django)   
-> Rank service(c++, communicate by fbthrift) -> Candidate account service load data from different sources  
-> Features used by machine learning 



### fbthrift

Instagram using fbthrift for internal information passing.  A fbthrift server has three kinds of threads: acceptor threads, I/O threads and worker threads.
- An acceptor thread accepts the client connection and assigns it to an I/O thread;
- The I/O thread reads the input data sent by a client, and passes it to a worker thread and the I/O thread will again be responsible for sending outbound requests later;
- The worker thread deserializes the input data into parameters, calls the request handler of the service in its context and spawns additional threads for outbound calls or computation.  The important part is that the thrift request handler runs in a worker thread and not in an I/O thread. This allows the server to be responsive to clients — even if all the worker threads are busy, the server will still have free I/O threads to send an overloaded response to clients and close sockets.


#### Synchronous I/O
loaded candidates and features synchronously.
- All the I/O calls were issued in parallel in separate threads(different request will have different I/O)
- At the end of the handler was a join() primitive which blocked until all the threads were done
- Which means, one worker thread for one client request
- One single request might break as many threads as the number of outbound calls

##### Disadvantage
It leads to a large memory footprint — each thread by default has a stack size of several MBs.
Too many thread needed: if each request will have M outbound calls, then N request need N * M threads
Thread scheduling also becomes a bottleneck in the kernel at around 400 threads.
With this model, we had to run several hundred instances of server across many machines to support our QPS, because we are not utilizing CPU resource or memory efficiently.


#### Using non-blocking I/O
fbthrift offers three ways to handle requests: synchronous, asynchronous and future-based.  Below is an API example generated by fbthrift.

```C++
class TestServiceSvIf : public TestServiceSvAsyncIf, public apache::thrift2::ServerInterface {
  ...
  virtual void sendResponse(std::string& _return, int64_t size);
  virtual void async_sendResponse(std::unique_ptr<apache::thrift2::HandlerCallback<std::unique_ptr<std::string>>> callback, int64_t size) = 0;
  virtual folly::Future<std::unique_ptr<std::string>> future_sendResponse(int64_t size);
  ...
}
```

non-blocking I/O 
- Every I/O thread has a list of file descriptors on whose status change it waits on in an event loop (it detects this status change through the select()/poll()/epoll() system call). 
- When the status of the file descriptor changes to “completed,” the I/O thread calls the associated callback. 
- Shared I/O thread, need client provide callback, I/O thread holds file descriptor corresponding to I/O operation(provided by eventlib)
 
##### Advantage
- Waiting on select()/poll()/epoll() puts a thread to sleep, which means it does not busy wait. Thus, it is efficient. 
To be clear, the synchronous I/O does not necessarily busy wait either, but it requires allocating one thread per I/O call.
- One I/O thread can take care of the I/O of multiple outbound requests. 
This reduces the memory footprint and synchronization costs associated with a large number of threads, and leads to a more scalable system.
- One worker thread does not need to wait for all the I/O associated with a single client request to complete before moving on to the next client request.  
Due to the communication between worker thread and outbound also using async strategy.
- **Thus, one worker thread can perform computation for multiple concurrent client requests.

##### Disadvantage
Callback Hell

<img src="../resource/instagram_future_callback_hell.png" alt="instagram_future_callback_hell.png" width="400"/>


#### Future chaining

From


<img src="../resource/instagram_future_from_callback.png" alt="instagram_future_from_callback.png" width="400"/>

To



<img src="../resource/instagram_future_to_then.png" alt="instagram_future_to_then.png" width="400"/>